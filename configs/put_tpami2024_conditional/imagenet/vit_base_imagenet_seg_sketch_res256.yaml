model:
  target: image_synthesis.modeling.models.masked_image_inpainting_transformer_seg_concat_sketch_unknow_dual_vqvae.MaskedImageInpaintingTransformerSegConcatUnknownClassSketch
  params:
    content_seq_len: 1024
    n_layer: 12 # number of transformer blocks
    dim: 768 # embedding dim
    num_heads: 12
    learn_mask_emb: True
    num_token: 8192
    act_layer: GELU2
    input_feature_type: origin

    attn_content_with_mask: False
    
    drop_path: 0.1 # follow PeCo
    random_quantize: 0.3
    weight_decay: 0.05

    unknow_class_param:
      num_unknow_classes: 20
      prob_unknow: 0.3
      max_number: 5
      unknown_class_start: 133

    seg_config_file: image_synthesis/modeling/mask2former/configs/coco/panoptic-segmentation/swin/maskformer2_swin_tiny_bs16_50ep.yaml
    seg_model_weight: /mnt/output/ckpt/mask2former/model_final_swin_t.pkl

    prob_sample:
      seg_prob: 0.25
      sketch_prob: 0.25
      no_guidance_prob: 0.25

    content_codec_config:
      target: image_synthesis.modeling.codecs.image_codec.patch_vqgan.PatchVQGAN
      params:
        # ckpt_path: OUTPUT_pretrain/p_vqvae_imagenet_gumbel_n8192n1024_zeromask0.95_bs28g9/checkpoint/000099e_508399iter.pth
        ckpt_path: /mnt/output/ckpt/p_vqvae_imagenet_n8192n1024_zeromask0.95_bs28g9/checkpoint/checkpoint/000099e_508399iter.pth
        trainable: False
        token_shape: [32, 32]
        im_process_info:
          scale: [255.0, 255.0, 255.0]
          mean: [0.485, 0.456, 0.406] # imagenet RGB mean
          std: [0.229, 0.224, 0.225] # imagenet RGB std
        quantizer_config:
          target: image_synthesis.modeling.codecs.image_codec.patch_vqgan.VectorQuantizer
          params:
            n_e: 9216 # 8192 + 1024
            e_dim: 256
            masked_embed_start: 8192
            embed_ema: True 
            get_embed_type: retrive
        encoder_config: 
          target: image_synthesis.modeling.codecs.image_codec.patch_vqgan.PatchEncoder2
          params:
            in_ch: 3
            res_ch: 256
            out_ch: 256
            num_res_block: 8
            stride: 8
        decoder_config:
          target: image_synthesis.modeling.codecs.image_codec.patch_vqgan.PatchConvDecoder2
          params:
            in_ch: 256
            out_ch: 3
            res_ch: 256
            num_res_block: 2
            num_res_block_after_resolution_change: 2
            stride: 8
            upsample_type: nearest
            up_layer_with_image: True
            add_noise_to_image: False

    sketch_param:
      ckpt_path: /mnt/output/ckpt/DexiNed/10_model.pth

    sketch_codec_config:
      target: image_synthesis.modeling.codecs.image_codec.patch_vqgan.PatchVQGAN
      params:
        ckpt_path: /mnt/output/pvqvae_sketch_imagenet_bs48g8_100e/checkpoint/000099e_333599iter.pth
        trainable: False
        token_shape: [32, 32]
        quantizer_config:
          target: image_synthesis.modeling.codecs.image_codec.patch_vqgan.VectorQuantizer
          params:
            n_e: 512
            e_dim: 256
            masked_embed_start: 512
            embed_ema: True
            get_embed_type: retrive
            distance_type: euclidean
        encoder_config: 
          target: image_synthesis.modeling.codecs.image_codec.patch_vqgan.PatchEncoder2
          params:
            in_ch: 1
            res_ch: 256
            out_ch: 256
            num_res_block: 8
            stride: 8
        decoder_config:
          target: image_synthesis.modeling.codecs.image_codec.patch_vqgan.PatchConvDecoder2
          params:
            in_ch: 256
            out_ch: 1
            res_ch: 256
            num_res_block: 2
            num_res_block_after_resolution_change: 2
            stride: 8
            up_layer_with_image: false
            upsample_type: nearest


    seg_codec_config:
      target: image_synthesis.modeling.codecs.image_codec.patch_vqgan.PatchVQGAN
        params:
          ckpt_path: /mnt/output/pvqvae_seg_imagenet_100e/checkpoint/last.pth
          trainable: False
          token_shape: [32, 32]
          im_process_info:
            scale: [76.5]
            mean: [1.0] # imagenet RGB mean
            std: [1.0]

          quantizer_config:
            target: image_synthesis.modeling.codecs.image_codec.patch_vqgan.VectorQuantizer
            params:
              n_e: 512
              e_dim: 256
              masked_embed_start: 512
              embed_ema: True
              get_embed_type: retrive
              distance_type: euclidean
              gumbel_sample: false
          
          encoder_config: 
            target: image_synthesis.modeling.codecs.image_codec.patch_vqgan.PatchEncoder2
            params:
              in_ch: 1
              res_ch: 256
              out_ch: 256
              num_res_block: 8
              stride: 8
          decoder_config:
            target: image_synthesis.modeling.codecs.image_codec.patch_vqgan.PatchConvDecoder2
            params:
              in_ch: 256
              out_ch: 1
              res_ch: 256
              num_res_block: 2
              num_res_block_after_resolution_change: 2
              stride: 8
              up_layer_with_image: false
              upsample_type: nearest


solver: # bs8g48
  find_unused_parameters: False
  base_lr: 0.0
  adjust_lr: none # not adjust lr according to total batch_size
  max_epochs: 300
  save_epochs: 20 #5 #TODO20
  validation_epochs: 10 #5 #TODO 5
  sample_iterations: 33362  #80070 # 20000 # TODO # how many iterations to perform sampling once ?
  optimizers_and_schedulers: # a list of configures, so we can config several optimizers and schedulers
  - name: transformer # default is None
    optimizer:
      target: torch.optim.AdamW
      params: 
        betas: !!python/tuple [0.9, 0.999] # follow PeCo
    scheduler:
      step_iteration: 1
      target: image_synthesis.engine.lr_scheduler.CosineAnnealingLRWithWarmup
      params:
        min_lr: 1.0e-5
        warmup_lr: 1.5e-3 # 5.0e-4 for BEiT, 1.5e-3 for PeCo # the lr to be touched after warmup
        warmup: 16681 # 80070 # 5 epochs 


dataloader:
  data_root: data
  batch_size: 8
  num_workers: 2
  train_datasets:
    - target: image_synthesis.data.image_list_dataset.ImageListDataset
      params:
        name: imagenet/train
        image_end_with: JPEG
        provided_mask_name: irregular-mask/testing_mask_dataset
        use_provided_mask_ratio: ['0.2', '0.6']
        use_provided_mask: 0.4
        mask: 1.0
        mask_low_to_high: 0.2
        mask_low_size: [32, 32]
        multi_image_mask: False 
        load_segmentation_map: True
        load_sketch_map: True
        return_data_keys: [image, mask, segmentation_map, sketch_map]
        stroken_mask_params:
          maxVertex: 10
          minVertex: 5
          maxLength: 100
          maxBrushWidth: 30
          minBrushWidth: 10
          keep_ratio: [0.25, 0.5]
          min_area: 64 # 8*8 we set the receptive field as the min masked area
        im_preprocessor_config:
          target: image_synthesis.data.utils.image_preprocessor.SimplePreprocessor
          params:
            size: [256, 256]
            smallest_max_size: 264
            random_crop: True
            horizon_flip: True
  validation_datasets:
    - target: image_synthesis.data.image_list_dataset.ImageListDataset
      params:
        name: imagenet/val
        image_end_with: JPEG
        provided_mask_name: irregular-mask/testing_mask_dataset
        use_provided_mask_ratio: ['0.2', '0.6']
        use_provided_mask: 0.4
        mask: 1.0
        mask_low_to_high: 0.0
        mask_low_size: [32, 32]
        multi_image_mask: False 
        load_segmentation_map: True
        load_sketch_map: True
        return_data_keys: [image, mask, segmentation_map, sketch_map]
        stroken_mask_params:
          maxVertex: 10
          minVertex: 5
          maxLength: 100
          maxBrushWidth: 30
          minBrushWidth: 10
          keep_ratio: [0.25, 0.5]
          min_area: 64 # 8*8 we set the receptive field as the min masked area
        
        im_preprocessor_config:
          target: image_synthesis.data.utils.image_preprocessor.SimplePreprocessor
          params:
            size: [256, 256]
            smallest_max_size: 256